{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\Desktop\\github\\ChatLongDoc\\utils.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m  \u001b[38;5;66;03m# 用于操作系统级别的调用，比如文件路径操作\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m  \u001b[38;5;66;03m# 用于哈希加密功能\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m  \u001b[38;5;66;03m# 指定如何将文本转换为tokens\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt35\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m makedata, url, headers\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tiktoken\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This is the public API of tiktoken\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoding \u001b[38;5;28;01mas\u001b[39;00m Encoding\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m encoding_for_model \u001b[38;5;28;01mas\u001b[39;00m encoding_for_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m encoding_name_for_model \u001b[38;5;28;01mas\u001b[39;00m encoding_name_for_model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tiktoken\\core.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractSet, Collection, Literal, NoReturn, Optional, Union\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _tiktoken\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEncoding\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter the file path or URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_path = input(\"Enter the file path or URL: \")\n",
    "text_path = \"example/example.pdf\"\n",
    "# text_path = \"https://arxiv.org/abs/1706.03762\"\n",
    "\n",
    "text = get_text(text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorize the data, the same document will only be processed once and cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cached memories in memory/315b4f4c655530196ef5f4f6f1d1f0b57d24fa2b08ff006c44b535d50499b487.json\n"
     ]
    }
   ],
   "source": [
    "memory_path = memorize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question: What is this article about?\n",
      "\n",
      "\u001b[92mThis article is about a new neural network architecture called the Transformer, which is based solely on attention mechanisms and dispenses with recurrent and convolutional neural networks. The Transformer has been shown to perform better in quality than existing models, including ensembles, while being more parallelizable and requiring significantly less time to train. The article discusses experiments on two machine translation tasks, achieving a new state of the art in both English-to-German and English-to-French translation. Additionally, the Transformer generalizes well to other tasks such as English constituency parsing. The article also includes visualizations of the attention mechanism used in the Transformer. The authors plan to apply the Transformer to other tasks and extend it to problems involving input and output modalities other than text. The code used for the experiments is available on GitHub.\u001b[0m\n",
      "\n",
      "Enter your question: 这篇文章讲了什么？\n",
      "\n",
      "\u001b[92m这篇文章介绍了一种新的神经网络模型——Transformer，它完全基于注意力机制，放弃了复杂的循环或卷积神经网络，能够更快地训练，更好地处理长序列。文章还提到，在机器翻译任务中，Transformer的表现比以往的模型要好，取得了最新的最好结果。文章还探讨了Transformer在英语成分句法分析任务中的应用，并取得了良好的结果。注意力机制也被证明在序列建模和转换模型中具有广泛的应用价值。\u001b[0m\n",
      "\n",
      "Enter your question: How self-attention is calculated?\n",
      "\n",
      "\u001b[92mSelf-attention is calculated using multi-head attention. In the encoder and decoder stacks, each layer has two sub-layers, the first being a multi-head self-attention mechanism. In multi-head attention, the queries, keys, and values are linearly projected h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys, and values, attention function is performed in parallel, yielding dv-dimensional output values that are concatenated and projected resulting in final values. The attention function can be described as mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Scaled Dot-Product Attention is used for multi-head attention in the Transformer model, where the dot products of the query with all keys are computed, divided each by √dk, and apply a softmax function to obtain the weights on the values.\u001b[0m\n",
      "\n",
      "Enter your question: 自注意力是如何计算的？\n",
      "\n",
      "\u001b[92mSelf-attention is computed using a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network in each layer of the encoder, with a residual connection and layer normalization around each sub-layer. The self-attention mechanism maps a query and a set of key-value pairs to an output using a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. The positional encoding used in the model corresponds to a sinusoid with the wavelengths forming a geometric progression. The self-attention layers have a total computational complexity that is faster than recurrent layers when the sequence length is smaller than the representation dimensionality, and the maximum path length between any two input and output positions is constant. The model is trained using the Adam optimizer and the training data consists of sentence pairs encoded using byte-pair encoding or word-piece vocabulary. The attention mechanism is observed to exhibit behavior related to the syntactic and semantic structure of the sentences.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat(memory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
